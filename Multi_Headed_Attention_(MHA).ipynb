{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzJhz_aSgX1",
        "outputId": "4bacc018-8ea6-46b7-9a17-acbfe129ec3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cpython' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "import math  # This imports the math module ,which provides access to mathemactical functions\n",
        "!git clone https://github.com/python/cpython.git\n",
        "from typing import List # use for type hinting\n",
        "\n",
        "import torch  # use for deep learning\n",
        "from torch import nn # from PyTorch which contains classes and funcations to build nerural networks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install labml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aLxFJ2KU3Kv",
        "outputId": "b2d4ebf7-126e-460c-87d5-f5486b5014c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting labml\n",
            "  Downloading labml-0.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gitpython (from labml)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from labml) (6.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from labml) (1.25.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->labml)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->labml)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading labml-0.5.2-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.9/110.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython, labml\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 labml-0.5.2 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from labml import tracker # USED  for tracking and visualizing machine learning experiments"
      ],
      "metadata": {
        "id": "NkEQizEeXFNN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwWDdpyuXNuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare for multi-head attention\n"
      ],
      "metadata": {
        "id": "dllwK2AQXQoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This module does a linear transformation and splits the vector into given number of heads for multi-head attention. This is used to transform key, query, and value vectors."
      ],
      "metadata": {
        "id": "O4yOzacoXZJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PrepareForMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
        "        super().__init__()\n",
        "        # Linear layer for linear transform\n",
        "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
        "        # Number of heads\n",
        "        self.heads = heads\n",
        "        # Number of Dimensions in vectors in each head\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor): # 'X' was changed to 'x' for consistency\n",
        "        # Input has shape [seq_len, batch_size, d_model] or [batch_size, d_model].\n",
        "        # We apply the linear transformation to the last dimension and split that into the heads\n",
        "        head_shape = x.shape[:-1]\n",
        "        # Linear transform\n",
        "        x = self.linear(x)\n",
        "        # Split last dimension into heads\n",
        "        x = x.view(*head_shape, self.heads, self.d_k)\n",
        "        # Output has shape [seq_len, batch_size, heads, d_k] or [batch_size, heads, d_model]\n",
        "        return x"
      ],
      "metadata": {
        "id": "KnotMlswXTF4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Head Attention Module"
      ],
      "metadata": {
        "id": "VeoCJ0oLZqqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This explains the computation of scaled multi-headed attention for given query, key, and value vectors.\n",
        "\n",
        "### Attention(Q, K, V) = softmax((QK^T) / √d_k) V\n",
        "\n",
        "In simple terms, it finds keys that match the query and retrieves the corresponding values. Here's a breakdown of the process:\n",
        "\n",
        "1. **Dot Product**: It uses the dot product of the query (Q) and key (K) vectors to determine how well they match.\n",
        "2. **Scaling**: Before applying softmax, the dot products are scaled by the square root of the dimension of the key vector (√d_k). This scaling prevents large dot-product values from causing the softmax function to output very small gradients when the key vector's dimension (d_k) is large.\n",
        "3. **Softmax**: The softmax function is applied to the scaled dot products to obtain a distribution over the keys, calculated along the sequence (or time) axis.\n",
        "4. **Weighted Sum**: The resulting weights from the softmax are used to compute a weighted sum of the value (V) vectors.\n",
        "\n",
        "In essence, the attention mechanism identifies relevant keys that correspond to a given query and uses these keys to retrieve the associated values, allowing the model to focus on specific parts of the input sequence."
      ],
      "metadata": {
        "id": "i2sOxoPPaGF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  #heads is the number of heads\n",
        "  #d_model is the number of features in the query,key and value vectors\n",
        "  def __init__(self,heads:int,d_model:int,dropout_prob:float=0.1,bias:bool=True):\n",
        "    super().__init__()\n",
        "    #number of features per head\n",
        "    self.d_k=d_model//head_shape\n",
        "    #number of heads\n",
        "    self.heads=heads\n",
        "    #these transform the query,key and value vectors for multi-headed attention\n",
        "    self.query=PrepareForMultiHeadAttention(d_model,heads,self.d_k,bias=bias)\n",
        "    self.key=PrepareForMultiHeadAttenation(d_model,heads,self.d-K,bias=bias)\n",
        "    self.value=PrepareForMultiHeadAttention(d_model,heads,self.d_k,bias=True)\n",
        "    # softmax for attention along the time dimension of key\n",
        "    self.softmax=nn.softmax(dim=1)\n",
        "\n",
        "    #output layer\n",
        "    self.output=nn.Linear(d-model,d_model)\n",
        "\n",
        "    #Dropout\n",
        "    self.dropout=nn.Dropout(dropout_prob)\n",
        "    # scaling factor before the softmax\n",
        "    self.scale=1/math.sqrt(self.d_k)\n",
        "\n",
        "    #we store attentions so that it can be used for logging ,or other computations if needed\n",
        "    self.attn=None\n",
        ""
      ],
      "metadata": {
        "id": "8Lw_p8INXh_a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate scores between queries and keys"
      ],
      "metadata": {
        "id": "pCQOLI3icyxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the dot product \\( QK^\\top \\) or \\( S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd} \\), we follow these steps:\n",
        "\n",
        "1. **Notations**:\n",
        "   - \\( Q \\): Query matrix with shape \\([batch, seq\\_len\\_q, num\\_heads, depth\\_q]\\)\n",
        "   - \\( K \\): Key matrix with shape \\([batch, seq\\_len\\_k, num\\_heads, depth\\_k]\\)\n",
        "   - \\( S \\): Output matrix after computing dot product of \\( Q \\) and \\( K \\), with shape \\([batch, seq\\_len\\_q, seq\\_len\\_k, num\\_heads]\\)\n",
        "\n",
        "2. **Dot Product Calculation**:\n",
        "   - For each batch \\( i \\), sequence positions \\( j \\) and \\( k \\), and head \\( b \\), we compute:\n",
        "     \\[\n",
        "     S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd}\n",
        "     \\]\n",
        "\n",
        "Here's a step-by-step breakdown:\n",
        "\n",
        "1. **Ensure the dimensions match**:\n",
        "   - The depth dimensions of \\( Q \\) and \\( K \\) should be the same, i.e., \\( depth\\_q = depth\\_k \\).\n",
        "\n",
        "2. **Element-wise Multiplication and Summation**:\n",
        "   - For each combination of \\( i, j, b, \\) and \\( h \\), compute the dot product by summing the products of corresponding elements in the \\( d \\) dimension of \\( Q \\) and \\( K \\).\n",
        "\n",
        "Here is a Python implementation using NumPy:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def compute_attention_scores(Q, K):\n",
        "    # Q shape: [batch, seq_len_q, num_heads, depth]\n",
        "    # K shape: [batch, seq_len_k, num_heads, depth]\n",
        "    \n",
        "    # Transpose K to match dimensions for dot product\n",
        "    K_transposed = np.transpose(K, (0, 2, 3, 1))  # Shape: [batch, num_heads, depth, seq_len_k]\n",
        "    \n",
        "    # Compute dot product\n",
        "    S = np.matmul(Q, K_transposed)  # Shape: [batch, seq_len_q, num_heads, seq_len_k]\n",
        "    \n",
        "    # Transpose S to the final shape\n",
        "    S = np.transpose(S, (0, 1, 3, 2))  # Shape: [batch, seq_len_q, seq_len_k, num_heads]\n",
        "    \n",
        "    return S\n",
        "\n",
        "# Example usage\n",
        "batch_size = 2\n",
        "seq_len_q = 3\n",
        "seq_len_k = 4\n",
        "num_heads = 5\n",
        "depth = 6\n",
        "\n",
        "Q = np.random.rand(batch_size, seq_len_q, num_heads, depth)\n",
        "K = np.random.rand(batch_size, seq_len_k, num_heads, depth)\n",
        "\n",
        "S = compute_attention_scores(Q, K)\n",
        "print(S.shape)  # Should print: (2, 3, 4, 5)\n",
        "```\n",
        "\n",
        "In this implementation:\n",
        "- We first transpose the key matrix \\( K \\) to align its dimensions for the dot product operation.\n",
        "- We then perform the matrix multiplication of \\( Q \\) and the transposed \\( K \\).\n",
        "- Finally, we transpose the resulting matrix \\( S \\) to get it in the desired shape."
      ],
      "metadata": {
        "id": "caihJDSldhMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this method can be overridden for other variations like relative attention"
      ],
      "metadata": {
        "id": "p_su_PPPc5Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores(self,query:torch.Tensor,key:torch.Tensor):\n",
        "  return torch.einsum('ibhd,jbhd->ijbh',query,key)\n",
        "  #mask has shape [seq_len_q,seq_len_k,batch_size],where first dimension is the query dimension. if the query dimension is equal to 1 it will be broadcasted"
      ],
      "metadata": {
        "id": "ITQlbUIGcw6u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_mask(self,mark:torch.Tensor,query_shape:List[int],key_shape:List[int]):\n",
        "  assert mask.shape[0]==1 or mask.shape[0] == query_shape[0]\n",
        "  assert mask.shape[1]==key_shape[0]\n",
        "  assert mask.shape[2]==1 or mask.shape[2]==query_shape[1]\n",
        "\n",
        "  # same mask applied to all heads\n",
        "  mask =mask.unsqueeze(-1)\n",
        "  # resulting mask has shape [aeq_len_q,seq_len_k,batch_size,heads]\n",
        "  return mask\n",
        ""
      ],
      "metadata": {
        "id": "_PRPo5ZAecge"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query,key and value are the tensors that store collection of query ,key and value vectors.\n",
        "from typing import Optional  # Import Optional for type hinting\n",
        "\n",
        "#They have shape [seq_len,batch_size,d_model].\n",
        "# mask has shape [seq_len,seq_len,batch_size] and mask[i,j,b] indicates whether for batch b,query at postition i has access to key-Value at postionj .\n",
        "def forward(self,*,\n",
        "            query:torch.Tensor,\n",
        "            key:torch.Tensor,\n",
        "            value:torch.Tensor,\n",
        "            mask:Optional[torch.Tensor]=None):\n",
        "  #query,key and value have shape[seq_len,batch_size,d_model]\n",
        "     seq_len,batch_size,_=query.key_shape\n",
        "     if mask is not None:\n",
        "      mask =self.prepare_mask(mask,query.shape,key.shape)\n",
        "\n",
        "      #Prepare query,key and value for attention computation.these will then have shape\n",
        "      #[seq_len,batch_size,heads,d_k]\n",
        "      query=self.query(query)\n",
        "      key=self.key(key)\n",
        "      value=self.value(value)\n",
        "\n",
        "      #compute attention QK^T .this gives a tensor of shape\n",
        "      #[seq_len,seq_len,batch_size,self.heads].\n",
        "\n",
        "      scores=self.get_scores(query,key)\n",
        "      #scale scores QK^T/root(dk)\n",
        "      scores *=self.scale\n",
        "      #apply mask\n",
        "      if mask is not None:\n",
        "        scores=scores.masked_fill(mask==0,float('-inf'))\n",
        "\n",
        "\n",
        "      attn=self.softmax(scores)\n",
        "      #save attentions if debugging\n",
        "      tracker.debug('attn',attn)\n",
        "\n",
        "      #apply dropout\n",
        "\n",
        "      attn=self.dropout(attn)\n",
        "      #multiply by values\n",
        "\n",
        "      x=torch.einsum('ijbh,jbhd->ibhd',attn,value)\n",
        "\n",
        "      # save attentions for any other calculations\n",
        "      self.attn=attn.detach()\n",
        "      # concatenate multiple heads\n",
        "      x=x.reshape(seq_len,batch_size,-1)\n",
        "      #output layer\n",
        "      x=self.output(x)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "O2f74dCtfg-I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrSouc27kSTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}